{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepTC_images_tracks_sync.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aachen6/deepTC/blob/master/deepTC_images_tracks_sync.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "x-OsbrIhbLLD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DeepTC - Extraction of the Dataset\n",
        "\n",
        "The objective of deepTC can be found on [deepTC github page](https://github.com/aachen6/deepTC/). This is the first Colab notebook for deepTC, where the best track and the satellite images of historical TCs will be extracted from website of the National Ocean and Atmosphere Agency (NOAA) and the US Naval Research Laboratory (NRL), respectively. The focus will be on the hurricanes from the Altantic Ocean and the infrared (IR) satellite images will be used.\n",
        "\n",
        "1. **[Satellite images and tracks of TCs](https://)**\n",
        "\n",
        "2. [Statistics of satellite images and tracks](https://)\n",
        "\n",
        "3. [Post-binding architecture for deepTC](https://)\n",
        "\n",
        "4. [CNN/Resnet model for deepTC](https://)\n",
        "\n",
        "5. [GAN model for deepTC](https://)\n",
        "\n",
        "6. [LSTM model for deepTC](https://)\n",
        " \n",
        "7. [LSTM-CNN model for deepTC](https://)\n",
        "\n",
        "\n",
        "Let's get started by loading some python modules, and for simplicity and efficiency due to the use of Google Colab, everthying will be extracted to and hosted on my Google Drive. A version of copy can be found on the [deepTC githubt page](https://github.com/aachen6/deepTC/). "
      ]
    },
    {
      "metadata": {
        "id": "-Q1UX4nda89c",
        "colab_type": "code",
        "outputId": "6ff65a6d-66e0-43ae-bf0a-f7b1fa1c80ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# basic module\n",
        "import os\n",
        "import yaml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime \n",
        "\n",
        "# web scraper\n",
        "import re \n",
        "import urllib\n",
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "# file and image management\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/My Drive/Colab Notebooks/deepTC'\n",
        "p_data = path + os.sep + 'data/AL'\n",
        "p_image = path + os.sep + 'image/AL'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1mJD5XONcFrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Best Tracks Extraction\n",
        "The best track information [HURDAT2]((https://www.nhc.noaa.gov/data/#hurdat) is available from NOAA National Hurricane Center (NHC). It's a comma-delimited, text format with six-hourly information on the location, maximum winds, central pressure, and (beginning in 2004) size of all known tropical cyclones and subtropical cyclone. The code below extracts the information, which is saved as a binary json file."
      ]
    },
    {
      "metadata": {
        "id": "GPfIPEzICmmh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tracks = []\n",
        "url_hurdat2 = r\"https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2017-050118.txt\"\n",
        "response = urllib.request.urlopen(url_hurdat2)\n",
        "\n",
        "for line in response: # loop through hurdat2 file \n",
        "  line = line.decode('utf-8').rstrip()\n",
        "  temp = line.replace(\" \", \"\").split(\",\")\n",
        "  if len(temp)==4: # storm header - new storm evetn\n",
        "    sid = temp[0]  # reorder storm keys as region, year, number, name\n",
        "    storm_id = sid[0:2] + sid[4:8] + sid[2:4] + sid[8:] \n",
        "    storm_key = storm_id + temp[1]\n",
        "  else: # time step of the storms\n",
        "    date_str = temp[0] + temp[1]\n",
        "    date = datetime.strptime(date_str, \"%Y%m%d%H%M\")\n",
        "    [cat, lat, lon, win, pre] = temp[3:8]\n",
        "    sgn = 1 if lat[-1]==\"N\" else -1 \n",
        "    lat = sgn*float(lat[0:-1])\n",
        "    sgn = 1 if lon[-1]==\"E\" else -1\n",
        "    lon = sgn*float(lon[0:-1])\n",
        "    win = float(win)\n",
        "    pre = float(pre)         \n",
        "    tracks.append([storm_key, date, cat, lat, lon, win, pre])\n",
        "    break\n",
        "\n",
        "f_out = p_data + os.sep + \"al_hurdat2.msg\"\n",
        "pd_track = pd.DataFrame(tracks, columns=[\"storm\",\"date\",\"cat\",\"lat\",\"lon\",\"win\",\"pre\"])\n",
        "pd_track.to_msgpack(f_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o_bRkSByDn1Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Satellite Image URLs Extraction\n",
        "NRL hosts the historical satellite images [tcdat](https://www.nrlmry.navy.mil/tcdat/) of TCs since 1997 at a resolution of 15 minutes. The data is organized by years, region,  storm name, satellite image type (visible, infrared etc.). A function is firstly defined to extract the satellite images by year, region, and image type. BeautifulSoup is used to parse the web content and identify the image URLs. At this moment, only the URLs will be archived and images will be download later to avoid unnecessary information. "
      ]
    },
    {
      "metadata": {
        "id": "sqlvGZR9D8eO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# constant variable\n",
        "BASE_URL = r\"https://www.nrlmry.navy.mil/tcdat/\"\n",
        "REGIONS  = [\"ATL\", \"CPAC\", \"EPAC\", \"WPAC\"]\n",
        "\n",
        "def bs_storm_img_by_year(year, region, img_type):\n",
        "  \n",
        "  o_storm_imgs = []\n",
        "\n",
        "  # storm key\n",
        "  key_region = region[0:3:2] if region==\"ATL\" else region[0:2]\n",
        "  \n",
        "  # parse storm page in specified years\n",
        "  url_year   = BASE_URL + \"tc{0}/\".format(year)\n",
        "  url_region = url_year + region + '/'\n",
        "  htm_region = requests.get(url_region)\n",
        "  bs_region  = bs(htm_region.content, 'html.parser')\n",
        "\n",
        "  # loop through storms\n",
        "  for a_storm in bs_region.find_all('a', href=True):\n",
        "    str_storm = a_storm['href'].split(\"/\")[0]\n",
        "    if not re.match('^[0-9]+', str_storm): continue # not a storm folder\n",
        "    key_base = key_region + str_storm.split(\".\")[0][0:2] + str_storm.split(\".\")[1]\n",
        "    \n",
        "    # check bw avail.\n",
        "    url_img = url_region + str_storm + '/{0}/geo/1km_bw/'.format(img_type)  \n",
        "    htm_img = requests.get(url_img)\n",
        "    if htm_img.status_code!=200: # file contains no bw\n",
        "      url_img = url_region + str_storm + '/{0}/geo/1km/'.format(img_type)  \n",
        "      htm_img = requests.get(url_img) \n",
        "    else:\n",
        "      bs_bad = bs(htm_img.content, 'html.parser')\n",
        "      if bs_bad.title.string==\"oops!\":\n",
        "        url_img = url_region + str_storm + '/{0}/geo/1km/'.format(img_type)  \n",
        "        htm_img = requests.get(url_img) \n",
        "\n",
        "    # parse page with images \n",
        "    bs_img = bs(htm_img.content, 'html.parser')\n",
        "  \n",
        "    # loop through image sets \n",
        "    for a_img in bs_img.find_all('a', href=True):\n",
        "      str_img = a_img['href'].split(\"/\")[0]\n",
        "      if \"jpg\" not in str_img: continue # not a satellite image  \n",
        "      if not re.match('^[0-9]{4}', str_img): continue # not a storm \n",
        "      url_jpg   = url_img + str_img # url to the satllite image\n",
        "      str_date  = str_img.split(\".\")[0] + str_img.split(\".\")[1]\n",
        "      storm_key = key_base[0:2] + str_date[0:4] + key_base[2:]\n",
        "      try: date = datetime.strptime(str_date, \"%Y%m%d%H%M\")\n",
        "      except: continue \n",
        "      o_storm_imgs.append([storm_key, date, url_jpg])\n",
        "      \n",
        "  return o_storm_imgs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hMlRgQTUTPNI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's now scrape the NRL satellite image archives and get all Altantic Ocean infrared satellite image URLs. The data is also saved in a binary json file."
      ]
    },
    {
      "metadata": {
        "id": "4N1YAbhRTQzf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "region   = \"ATL\"   # Atlantic only \n",
        "img_type = \"ir\"  # use infra-red \n",
        "years    = [str(x%100).rjust(2,\"0\") for x in range(1997, 2018)]\n",
        "\n",
        "storm_imgs = []\n",
        "for y in years:\n",
        "  print (\"processing year {0}\".format(y))\n",
        "  _imgs = bs_storm_img_by_year(y, region, img_type)\n",
        "  storm_imgs = storm_imgs + _imgs\n",
        "    \n",
        "f_img = p_data + os.sep + \"al_ir_urls.msg\"\n",
        "pd_img = pd.DataFrame(storm_imgs, columns=[\"storm\", \"date\", \"url\"])\n",
        "pd_img.to_msgpack(f_img)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cVXSzbk56Qsh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Satellite Image and Best Track Sychronization\n",
        "\n",
        "The Hurdat2 contains following classifications of the storm status:\n",
        "\n",
        "* TD – Tropical cyclone of tropical depression intensity (< 34 knots)\n",
        "* TS – Tropical cyclone of tropical storm intensity (34-63 knots)\n",
        "* HU – Tropical cyclone of hurricane intensity (> 64 knots)\n",
        "* EX – Extratropical cyclone (of any intensity)\n",
        "* SD – Subtropical cyclone of subtropical depression intensity (< 34 knots)\n",
        "* SS – Subtropical cyclone of subtropical storm intensity (> 34 knots)\n",
        "* LO – A low that is neither a tropical cyclone, a subtropical cyclone, nor an extratropical cyclone (of any intensity)\n",
        "* DB – Disturbance (of any intensity) \n",
        "\n",
        "The hurricane HU can be further classfied into five categories based on the Saffir–Simpson hurricane wind scale (SSHWS). The function below converts HU into five hurricane categories based on the maximum sustained wind speed. If the track information at next time step is passed as an argument, it also performs a check if the status of storm classification remains the same. The purpose is to expand the satellite image dataset by utilization images that occurred between 6-hour interval best track timeframe, since it has a resolution of 15 minutes.  \n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "tM2CdWJ56QTo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def saffir_simpson(info_l, info_r=None):\n",
        "\n",
        "  wins  = np.array([64, 83, 96, 113, 137, 9999])\n",
        "  \n",
        "  def cat(_info):\n",
        "    if _info[0]==\"HU\":\n",
        "      diff  = wins - _info[1]\n",
        "      n_cat = np.argwhere(diff>0)[0][0]\n",
        "      _info[0] = \"HU\" + str(n_cat) \n",
        "    \n",
        "    return _info\n",
        "\n",
        "  t_info = cat(info_l)\n",
        "\n",
        "  # if pass the track information at next time step, \n",
        "  # will check if status remains the same  \n",
        "  if info_r is not None and cat(info_r)[0]!=t_info[0]:\n",
        "    t_info = None  # catergories changed, return None\n",
        "\n",
        "  return t_info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5t5_VmhL61q0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "The best track information need be mapped onto the satellite images based on its occurence time. However, these two datasets are not synchronized and have different temporal resolutions with the track information at 6-hour interval while the satellite images at 15-mininute interval. To expand the datasets, satellite images fall within two consective track times with the same classfication are assigned with the same track information/classfication. This assumption in general holds since the storm system is persistent within that timeframe. For each satellite image in each storm, two consective times from the track information that bounds the image time are identifed and the storm characteristics are assigned to the image accordingly.\n"
      ]
    },
    {
      "metadata": {
        "id": "laxUskIw66EO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sync_track_to_image(pd_track, pd_image):\n",
        "\n",
        "  track_storm_keys = set(pd_track[\"storm\"].tolist()) # storms from track\n",
        "  image_storm_keys = set(pd_image[\"storm\"].tolist()) # storms from image\n",
        "  storm_keys = track_storm_keys & image_storm_keys   # common storms\n",
        "\n",
        "  sync = [] \n",
        "  for key in storm_keys:\n",
        "    print (\"processing storm \" + key)\n",
        "    ts_in_track = pd_track[pd_track[\"storm\"]==key]\n",
        "    ts_in_image = pd_image[pd_image[\"storm\"]==key]\n",
        "    \n",
        "    track_dates = ts_in_track[\"date\"].tolist()\n",
        "    track_infos = ts_in_track[[\"cat\",\"win\"]].values.tolist()\n",
        "\n",
        "    for index, row in ts_in_image.iterrows():\n",
        "      # check nearest track information before and after the image date \n",
        "      date = row[\"date\"] # image date and time\n",
        "      diff = np.array([(t-date).total_seconds()/3600. for t in track_dates])\n",
        "      idx_0 = np.argwhere(diff==0)\n",
        "      is_direct = False\n",
        "      if len(idx_0)!=0: # direct match time stamp\n",
        "        is_direct = True\n",
        "        _info = track_infos[idx_0[0][0]]\n",
        "        t_info = saffir_simpson(_info)\n",
        "        _url = row[\"url\"] \n",
        "      else: # indirect match\n",
        "        try: # retain only if nearest track information has the same category \n",
        "          idx_l = np.argwhere(diff<0)[-1][0]\n",
        "          idx_r = np.argwhere(diff>0)[0][0]\n",
        "          t_info = saffir_simpson(track_infos[idx_l], track_infos[idx_r]) \n",
        "          if t_info is None: continue \n",
        "        except:\n",
        "          continue \n",
        "          \n",
        "      try: # unique image file name key_datetime.jpg\n",
        "        _url = row[\"url\"]\n",
        "        _image = \"{0}_{1}.jpg\".format(key, row[\"date\"].strftime(\"%m%d%H%M\"))\n",
        "      except:\n",
        "        continue\n",
        " \n",
        "      sync.append([key, date] + t_info + [is_direct, _url, _image])\n",
        "            \n",
        "  return sync  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pP7BS3UK6-XF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we can sychronize the dataset and archive into a binary json file."
      ]
    },
    {
      "metadata": {
        "id": "w4T9UOF_6_13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load storm tracks\n",
        "f_track = r\"./data/al_hurdat2.msg\"\n",
        "pd_track = pd.read_msgpack(f_track)\n",
        "pd_track[\"date\"] = pd.to_datetime(pd_track[\"date\"],format=\"%Y-%m-%d %H%M%S\")\n",
        "\n",
        "# load storm satellite images\n",
        "f_image = \"./data/al_ir_urls.msg\"\n",
        "pd_image = pd.read_msgpack(f_image)\n",
        "\n",
        "# sync track and image for classification labels\n",
        "f_sync = \"./data/al_ir_track.msg\"\n",
        "sync = sync_track_to_image(pd_track, pd_image)\n",
        "sync.sort(key = lambda x: x[0])\n",
        "pd_sync = pd.DataFrame(sync, columns=[\"storm\",\"date\",\"cat\",\"win\",\"direct\",\"url\",\"image\"])\n",
        "pd_sync.to_msgpack(f_sync)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_kJ5iQe7FHT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Download and Archive the Images\n",
        "\n",
        "The function below downloads a single image with an extension to interpolate image at particular datetime based on two consective images with bounding datatime. We will skip this part since it takes a long time to download the entire image archive. The data has been downloaded locally and uploaded into the google drive."
      ]
    },
    {
      "metadata": {
        "id": "RYs86sqC7Pk7",
        "colab_type": "code",
        "outputId": "d2334afc-4ebe-4ff7-ae5b-6de0657495d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "print (os.getcwd()) \n",
        "print (\"../\")\n",
        "\n",
        "def download_blend_image(f_image, url_l, url_r=None, weight=None):\n",
        "\n",
        "  req = requests.get(url_l, stream=True)\n",
        "  try:\n",
        "    img = Image.open(BytesIO(req.content))\n",
        "    if url_r is not None:\n",
        "      req = requests.get(url_r, stream=True)\n",
        "      img_r = Image.open(BytesIO(req.content))\n",
        "      img = Image.blend(img_r, img, weight)   \n",
        "    img.save(f_image)\n",
        "  except:\n",
        "    raise ValueError(\"Fail to download\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "../\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}